#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
1.4.1.1 é›†æˆæµç¨‹æµ‹è¯• - ä»å›¾æŸ¥è¯¢ä¼˜åŒ–åˆ°OpenAIåˆ†æ
å±•ç¤ºé¡¹ç›®ä»1.3.2å›¾æŸ¥è¯¢ä¼˜åŒ–åˆ°1.4.1 OpenAIåˆ†æçš„å®Œæ•´æ•°æ®æµç¨‹

æµ‹è¯•æµç¨‹:
1. è¿æ¥Neo4jæ•°æ®åº“
2. ä½¿ç”¨1.3.2å›¾æŸ¥è¯¢ä¼˜åŒ–å™¨åˆ†æå½“å‰æ•°æ®åº“å†…å®¹
3. å°†æŸ¥è¯¢ç»“æœè¾“å…¥åˆ°1.4.1 OpenAIæœåŠ¡è¿›è¡Œæ™ºèƒ½åˆ†æ
4. è¾“å‡ºåˆ†æç»“æœ
"""

import pytest
import asyncio
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from loguru import logger

# æ·»åŠ é¡¹ç›®è·¯å¾„
import sys
sys.path.append('/home/xzj/01_Project/B_25OS/src/backend')

# å¯¼å…¥1.3.2å›¾æŸ¥è¯¢ä¼˜åŒ–æ¨¡å—
from services.graph_query_optimizer import (
    GraphQueryOptimizer,
    QueryOptimizerConfig,
    create_query_optimizer
)
from services.graph_database import GraphDatabaseManager
from services.interfaces import QueryType, OptimizationLevel

# å¯¼å…¥1.4.1 OpenAIåˆ†ææ¨¡å—
from services.openai_service import (
    OpenAIService,
    AnalysisRequest,
    AnalysisResponse,
    AnalysisType,
    Priority,
    analyze_events
)
from config.openai_config import OpenAIConfig

# å¯¼å…¥æ ¸å¿ƒé…ç½®
from core.config import settings
from core.database import neo4j_driver


class TestIntegrationFlow:
    """é›†æˆæµç¨‹æµ‹è¯•ç±»"""
    
    def __init__(self):
        self.db_manager = None
        self.query_optimizer = None
        self.openai_service = None
        self.test_results = {}
    
    async def setup_services(self):
        """åˆå§‹åŒ–æœåŠ¡ç»„ä»¶"""
        logger.info("=== å¼€å§‹åˆå§‹åŒ–æœåŠ¡ç»„ä»¶ ===")
        
        try:
            # 1. åˆå§‹åŒ–Neo4jæ•°æ®åº“è¿æ¥
            logger.info("1. åˆå§‹åŒ–Neo4jæ•°æ®åº“è¿æ¥...")
            await neo4j_driver.connect()
            connection_status = await neo4j_driver.verify_connectivity()
            
            if not connection_status:
                raise Exception("Neo4jæ•°æ®åº“è¿æ¥å¤±è´¥")
            
            logger.info("âœ… Neo4jæ•°æ®åº“è¿æ¥æˆåŠŸ")
            
            # 2. åˆå§‹åŒ–å›¾æ•°æ®åº“ç®¡ç†å™¨
            logger.info("2. åˆå§‹åŒ–å›¾æ•°æ®åº“ç®¡ç†å™¨...")
            self.db_manager = GraphDatabaseManager(
                uri=settings.neo4j_uri,
                username=settings.neo4j_user,
                password=settings.neo4j_password,
                database=settings.neo4j_database
            )
            
            # è¿æ¥æ•°æ®åº“ç®¡ç†å™¨
            await self.db_manager.connect()
            logger.info("âœ… å›¾æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–æˆåŠŸ")
            
            # 3. åˆå§‹åŒ–1.3.2å›¾æŸ¥è¯¢ä¼˜åŒ–å™¨
            logger.info("3. åˆå§‹åŒ–å›¾æŸ¥è¯¢ä¼˜åŒ–å™¨...")
            optimizer_config = QueryOptimizerConfig(
                enabled=True,
                optimization_level=OptimizationLevel.BALANCED,
                cache_enabled=True,
                cache_size=1000,
                cache_ttl=3600,
                query_timeout=30
            )
            
            self.query_optimizer = GraphQueryOptimizer(
                config=optimizer_config,
                db_manager=self.db_manager
            )
            
            # å¯åŠ¨å¹¶éªŒè¯æŸ¥è¯¢ä¼˜åŒ–å™¨
            await self.query_optimizer.start_optimizer()
            if not hasattr(self.query_optimizer, 'config'):
                raise AssertionError("æŸ¥è¯¢ä¼˜åŒ–å™¨é…ç½®ç¼ºå¤±")
            
            logger.info("âœ… å›¾æŸ¥è¯¢ä¼˜åŒ–å™¨å¯åŠ¨å¹¶éªŒè¯é€šè¿‡")
            
            # 4. åˆå§‹åŒ–1.4.1 OpenAIæœåŠ¡
            logger.info("4. åˆå§‹åŒ–OpenAIæœåŠ¡...")
            self.openai_service = OpenAIService()
            
            # éªŒè¯OpenAIæœåŠ¡
            service_stats = self.openai_service.get_statistics()
            if self.openai_service.client is None:
                raise Exception("OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥")
            
            logger.info(f"OpenAIæœåŠ¡ç»Ÿè®¡: {service_stats}")
            
            logger.info("âœ… OpenAIæœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
            logger.info("=== æ‰€æœ‰æœåŠ¡ç»„ä»¶åˆå§‹åŒ–å®Œæˆ ===")
            
        except Exception as e:
            logger.error(f"âŒ æœåŠ¡åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def analyze_database_content(self) -> Dict[str, Any]:
        """æ­¥éª¤1: ä½¿ç”¨1.3.2å›¾æŸ¥è¯¢ä¼˜åŒ–å™¨åˆ†ææ•°æ®åº“å†…å®¹"""
        logger.info("\n=== æ­¥éª¤1: åˆ†æNeo4jæ•°æ®åº“å†…å®¹ ===")
        
        try:
            # 1. è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯
            logger.info("1.1 è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯...")
            db_stats = await self.db_manager.get_graph_stats()
            logger.info(f"æ•°æ®åº“ç»Ÿè®¡: {json.dumps(db_stats, indent=2, ensure_ascii=False)}")
            
            # 2. æŸ¥è¯¢æœ€è¿‘çš„å®‰å…¨äº‹ä»¶
            logger.info("1.2 æŸ¥è¯¢æœ€è¿‘çš„å®‰å…¨äº‹ä»¶...")
            recent_events_query = """
            MATCH (e:Event)
            WHERE e.timestamp >= datetime() - duration('P7D')
            RETURN e.event_id, e.rule, e.message, e.priority, e.timestamp, e.source
            ORDER BY e.timestamp DESC
            LIMIT 20
            """
            
            recent_events = await self.query_optimizer.execute_optimized_query(
                query=recent_events_query,
                params={}
            )
            
            logger.info(f"æ‰¾åˆ° {len(recent_events.get('data', []))} ä¸ªæœ€è¿‘äº‹ä»¶")
            
            # 3. åˆ†æäº‹ä»¶å…³è”å…³ç³»
            logger.info("1.3 åˆ†æäº‹ä»¶å…³è”å…³ç³»...")
            correlation_query = """
            MATCH (e1:Event)-[r:TRIGGERS|FOLLOWS|CORRELATES_WITH]->(e2:Event)
            WHERE e1.timestamp >= datetime() - duration('P7D')
            RETURN e1.event_id, type(r) as relation_type, e2.event_id, 
                   e1.rule as source_rule, e2.rule as target_rule,
                   e1.priority as source_priority, e2.priority as target_priority
            ORDER BY e1.timestamp DESC
            LIMIT 15
            """
            
            correlations = await self.query_optimizer.execute_optimized_query(
                query=correlation_query,
                params={}
            )
            
            logger.info(f"æ‰¾åˆ° {len(correlations.get('data', []))} ä¸ªäº‹ä»¶å…³è”")
            
            # 4. æŸ¥è¯¢é«˜ä¼˜å…ˆçº§äº‹ä»¶
            logger.info("1.4 æŸ¥è¯¢é«˜ä¼˜å…ˆçº§äº‹ä»¶...")
            high_priority_query = """
            MATCH (e:Event)
            WHERE e.priority IN ['Critical', 'High', 'critical', 'high']
              AND e.timestamp >= datetime() - duration('P3D')
            RETURN e.event_id, e.rule, e.message, e.priority, e.timestamp,
                   e.source, e.process_name, e.user_name, e.file_path
            ORDER BY e.timestamp DESC
            LIMIT 10
            """
            
            high_priority_events = await self.query_optimizer.execute_optimized_query(
                query=high_priority_query,
                params={}
            )
            
            logger.info(f"æ‰¾åˆ° {len(high_priority_events.get('data', []))} ä¸ªé«˜ä¼˜å…ˆçº§äº‹ä»¶")
            
            # 5. åˆ†ææ”»å‡»è·¯å¾„
            logger.info("1.5 åˆ†ææ½œåœ¨æ”»å‡»è·¯å¾„...")
            attack_path_query = """
            MATCH path = (start:Event)-[:TRIGGERS*1..3]->(end:Event)
            WHERE start.priority IN ['Critical', 'High'] 
              AND start.timestamp >= datetime() - duration('P1D')
            RETURN 
                [node in nodes(path) | {
                    event_id: node.event_id,
                    rule: node.rule,
                    timestamp: node.timestamp,
                    priority: node.priority
                }] as attack_sequence,
                length(path) as path_length
            ORDER BY path_length DESC, start.timestamp DESC
            LIMIT 5
            """
            
            attack_paths = await self.query_optimizer.execute_optimized_query(
                query=attack_path_query,
                params={}
            )
            
            logger.info(f"æ‰¾åˆ° {len(attack_paths.get('data', []))} ä¸ªæ½œåœ¨æ”»å‡»è·¯å¾„")
            
            # æ•´åˆåˆ†æç»“æœ
            analysis_result = {
                'database_stats': db_stats,
                'recent_events': recent_events.get('data', []),
                'event_correlations': correlations.get('data', []),
                'high_priority_events': high_priority_events.get('data', []),
                'attack_paths': attack_paths.get('data', []),
                'query_performance': {
                    'recent_events_time': recent_events.get('execution_time', 0),
                    'correlations_time': correlations.get('execution_time', 0),
                    'high_priority_time': high_priority_events.get('execution_time', 0),
                    'attack_paths_time': attack_paths.get('execution_time', 0)
                },
                'analysis_timestamp': datetime.now().isoformat()
            }
            
            self.test_results['graph_analysis'] = analysis_result
            logger.info("âœ… æ•°æ®åº“å†…å®¹åˆ†æå®Œæˆ")
            
            return analysis_result
            
        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“åˆ†æå¤±è´¥: {e}")
            raise
    
    async def analyze_with_openai(self, graph_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ­¥éª¤2: ä½¿ç”¨1.4.1 OpenAIæœåŠ¡åˆ†æå›¾æŸ¥è¯¢ç»“æœ"""
        logger.info("\n=== æ­¥éª¤2: OpenAIæ™ºèƒ½åˆ†æ ===")
        
        try:
            # 1. å‡†å¤‡åˆ†ææ•°æ®
            logger.info("2.1 å‡†å¤‡åˆ†ææ•°æ®...")
            
            # æå–å…³é”®äº‹ä»¶ç”¨äºåˆ†æ
            events_for_analysis = []
            
            # æ·»åŠ é«˜ä¼˜å…ˆçº§äº‹ä»¶
            for event in graph_data.get('high_priority_events', []):
                events_for_analysis.append({
                    'event_id': event.get('e.event_id'),
                    'rule': event.get('e.rule'),
                    'message': event.get('e.message'),
                    'priority': event.get('e.priority'),
                    'timestamp': event.get('e.timestamp'),
                    'source': event.get('e.source'),
                    'process_name': event.get('e.process_name'),
                    'user_name': event.get('e.user_name'),
                    'file_path': event.get('e.file_path')
                })
            
            # æ·»åŠ æœ€è¿‘äº‹ä»¶ï¼ˆå¦‚æœé«˜ä¼˜å…ˆçº§äº‹ä»¶ä¸è¶³ï¼‰
            if len(events_for_analysis) < 5:
                for event in graph_data.get('recent_events', [])[:10]:
                    event_data = {
                        'event_id': event.get('e.event_id'),
                        'rule': event.get('e.rule'),
                        'message': event.get('e.message'),
                        'priority': event.get('e.priority'),
                        'timestamp': event.get('e.timestamp'),
                        'source': event.get('e.source')
                    }
                    if event_data not in events_for_analysis:
                        events_for_analysis.append(event_data)
            
            logger.info(f"å‡†å¤‡åˆ†æ {len(events_for_analysis)} ä¸ªäº‹ä»¶")
            
            # 2. åˆ›å»ºåˆ†æè¯·æ±‚
            logger.info("2.2 åˆ›å»ºOpenAIåˆ†æè¯·æ±‚...")
            
            analysis_request = AnalysisRequest(
                analysis_type=AnalysisType.SECURITY_ANALYSIS,
                events=events_for_analysis,
                context={
                    'database_stats': graph_data.get('database_stats', {}),
                    'correlations_count': len(graph_data.get('event_correlations', [])),
                    'attack_paths_count': len(graph_data.get('attack_paths', [])),
                    'analysis_timeframe': '7 days',
                    'system_info': 'NeuronOS Security Monitoring System'
                },
                priority=Priority.HIGH
            )
            
            # 3. æ‰§è¡Œå®‰å…¨åˆ†æ
            logger.info("2.3 æ‰§è¡Œå®‰å…¨åˆ†æ...")
            security_analysis = await self.openai_service.analyze_security_events(analysis_request)
            
            logger.info("âœ… å®‰å…¨åˆ†æå®Œæˆ")
            logger.info(f"åˆ†æç»“æœæ¦‚è¦: {security_analysis.summary}")
            
            # 4. æ‰§è¡Œå¨èƒè¯„ä¼°ï¼ˆå¦‚æœæœ‰é«˜ä¼˜å…ˆçº§äº‹ä»¶ï¼‰
            threat_assessment = None
            if graph_data.get('high_priority_events'):
                logger.info("2.4 æ‰§è¡Œå¨èƒè¯„ä¼°...")
                
                threat_request = AnalysisRequest(
                    analysis_type=AnalysisType.THREAT_ASSESSMENT,
                    events=events_for_analysis[:5],  # åªåˆ†æå‰5ä¸ªäº‹ä»¶
                    context={
                        'high_priority_count': len(graph_data.get('high_priority_events', [])),
                        'attack_paths': graph_data.get('attack_paths', []),
                        'system_criticality': 'high'
                    },
                    priority=Priority.CRITICAL
                )
                
                threat_assessment = await self.openai_service.assess_threat(threat_request)
                logger.info("âœ… å¨èƒè¯„ä¼°å®Œæˆ")
                logger.info(f"å¨èƒè¯„ä¼°ç»“æœ: {threat_assessment.summary}")
            
            # 5. ç”Ÿæˆä¿®å¤å»ºè®®ï¼ˆå¦‚æœå‘ç°å¨èƒï¼‰
            remediation_advice = None
            if threat_assessment and threat_assessment.risk_score > 7.0:
                logger.info("2.5 ç”Ÿæˆä¿®å¤å»ºè®®...")
                
                remediation_request = AnalysisRequest(
                    analysis_type=AnalysisType.REMEDIATION_ADVICE,
                    events=events_for_analysis[:3],  # åªé’ˆå¯¹æœ€å…³é”®çš„äº‹ä»¶
                    context={
                        'threat_level': threat_assessment.risk_score,
                        'security_analysis': security_analysis.summary,
                        'system_type': 'Linux/OpenKylin',
                        'monitoring_tool': 'Falco'
                    },
                    priority=Priority.HIGH
                )
                
                remediation_advice = await self.openai_service.get_remediation_advice(remediation_request)
                logger.info("âœ… ä¿®å¤å»ºè®®ç”Ÿæˆå®Œæˆ")
            
            # æ•´åˆOpenAIåˆ†æç»“æœ
            openai_result = {
                'security_analysis': {
                    'summary': security_analysis.summary,
                    'risk_score': security_analysis.risk_score,
                    'confidence': security_analysis.confidence,
                    'detailed_analysis': security_analysis.detailed_analysis,
                    'recommendations': security_analysis.recommendations,
                    'attack_vectors': security_analysis.attack_vectors,
                    'mitigation_steps': security_analysis.mitigation_steps,
                    'processing_time': security_analysis.processing_time,
                    'token_usage': security_analysis.token_usage
                },
                'threat_assessment': {
                    'summary': threat_assessment.summary if threat_assessment else None,
                    'risk_score': threat_assessment.risk_score if threat_assessment else None,
                    'confidence': threat_assessment.confidence if threat_assessment else None,
                    'attack_vectors': threat_assessment.attack_vectors if threat_assessment else [],
                    'mitigation_steps': threat_assessment.mitigation_steps if threat_assessment else [],
                    'processing_time': threat_assessment.processing_time if threat_assessment else None
                } if threat_assessment else None,
                'remediation_advice': {
                    'summary': remediation_advice.summary if remediation_advice else None,
                    'mitigation_steps': remediation_advice.mitigation_steps if remediation_advice else [],
                    'recommendations': remediation_advice.recommendations if remediation_advice else [],
                    'attack_vectors': remediation_advice.attack_vectors if remediation_advice else []
                } if remediation_advice else None,
                'analysis_metadata': {
                    'events_analyzed': len(events_for_analysis),
                    'analysis_timestamp': datetime.now().isoformat(),
                    'total_processing_time': (
                        security_analysis.processing_time +
                        (threat_assessment.processing_time if threat_assessment else 0) +
                        (remediation_advice.processing_time if remediation_advice else 0)
                    ),
                    'total_tokens_used': (
                        security_analysis.token_usage.get('total', 0) +
                        (threat_assessment.token_usage.get('total', 0) if threat_assessment else 0) +
                        (remediation_advice.token_usage.get('total', 0) if remediation_advice else 0)
                    )
                }
            }
            
            self.test_results['openai_analysis'] = openai_result
            logger.info("âœ… OpenAIåˆ†æå®Œæˆ")
            
            return openai_result
            
        except Exception as e:
            logger.error(f"âŒ OpenAIåˆ†æå¤±è´¥: {e}")
            raise
    
    async def generate_comprehensive_report(self, graph_data: Dict[str, Any], openai_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ­¥éª¤3: ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
        logger.info("\n=== æ­¥éª¤3: ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š ===")
        
        try:
            # è®¡ç®—å…³é”®æŒ‡æ ‡
            total_events = len(graph_data.get('recent_events', []))
            high_priority_events = len(graph_data.get('high_priority_events', []))
            correlations = len(graph_data.get('event_correlations', []))
            attack_paths = len(graph_data.get('attack_paths', []))
            
            # è·å–OpenAIåˆ†æç»“æœ
            security_analysis = openai_data.get('security_analysis', {})
            threat_assessment = openai_data.get('threat_assessment')
            remediation_advice = openai_data.get('remediation_advice')
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = {
                'report_metadata': {
                    'generated_at': datetime.now().isoformat(),
                    'analysis_period': '7 days',
                    'system': 'NeuronOS Security Monitoring',
                    'version': '1.4.1'
                },
                'executive_summary': {
                    'total_events_analyzed': total_events,
                    'high_priority_events': high_priority_events,
                    'event_correlations': correlations,
                    'attack_paths_detected': attack_paths,
                    'overall_risk_score': security_analysis.get('risk_score', 0),
                    'confidence_level': security_analysis.get('confidence', 0),
                    'threat_level': 'High' if threat_assessment and threat_assessment.get('risk_score', 0) > 7 else 'Medium' if security_analysis.get('risk_score', 0) > 5 else 'Low'
                },
                'graph_analysis_summary': {
                    'database_statistics': graph_data.get('database_stats', {}),
                    'query_performance': graph_data.get('query_performance', {}),
                    'key_findings': {
                        'recent_activity': f"å‘ç° {total_events} ä¸ªæœ€è¿‘äº‹ä»¶",
                        'priority_events': f"è¯†åˆ« {high_priority_events} ä¸ªé«˜ä¼˜å…ˆçº§äº‹ä»¶",
                        'correlations': f"æ£€æµ‹åˆ° {correlations} ä¸ªäº‹ä»¶å…³è”",
                        'attack_patterns': f"å‘ç° {attack_paths} ä¸ªæ½œåœ¨æ”»å‡»è·¯å¾„"
                    }
                },
                'ai_analysis_summary': {
                    'security_analysis': security_analysis,
                    'threat_assessment': threat_assessment,
                    'remediation_advice': remediation_advice,
                    'processing_metrics': openai_data.get('analysis_metadata', {})
                },
                'recommendations': {
                    'immediate_actions': [],
                    'monitoring_improvements': [],
                    'security_enhancements': []
                },
                'technical_details': {
                    'graph_queries_executed': 5,
                    'ai_analyses_performed': 1 + (1 if threat_assessment else 0) + (1 if remediation_advice else 0),
                    'total_processing_time': (
                        sum(graph_data.get('query_performance', {}).values()) +
                        openai_data.get('analysis_metadata', {}).get('total_processing_time', 0)
                    ),
                    'data_sources': ['Neo4j Graph Database', 'OpenAI GPT-4', 'Falco Security Events']
                }
            }
            
            # æ·»åŠ å…·ä½“å»ºè®®
            if security_analysis.get('recommendations'):
                comprehensive_report['recommendations']['immediate_actions'].extend(
                    security_analysis['recommendations'][:3]
                )
            
            if threat_assessment:
                comprehensive_report['recommendations']['security_enhancements'].append(
                    f"å¨èƒè¯„ä¼°æ˜¾ç¤ºé£é™©è¯„åˆ†ä¸º {threat_assessment.get('risk_score', 0):.1f}ï¼Œå»ºè®®åŠ å¼ºç›‘æ§"
                )
            
            if remediation_advice:
                if remediation_advice.get('mitigation_steps'):
                    comprehensive_report['recommendations']['immediate_actions'].extend(
                        remediation_advice['mitigation_steps'][:2]
                    )
            
            comprehensive_report['recommendations']['monitoring_improvements'] = [
                "ä¼˜åŒ–å›¾æŸ¥è¯¢æ€§èƒ½ï¼Œå½“å‰å¹³å‡æŸ¥è¯¢æ—¶é—´è¾ƒé•¿",
                "å¢åŠ å®æ—¶äº‹ä»¶å…³è”åˆ†æ",
                "æ‰©å±•æ”»å‡»è·¯å¾„æ£€æµ‹ç®—æ³•"
            ]
            
            self.test_results['comprehensive_report'] = comprehensive_report
            logger.info("âœ… ç»¼åˆåˆ†ææŠ¥å‘Šç”Ÿæˆå®Œæˆ")
            
            return comprehensive_report
            
        except Exception as e:
            logger.error(f"âŒ æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    async def cleanup_services(self):
        """æ¸…ç†æœåŠ¡èµ„æº"""
        logger.info("\n=== æ¸…ç†æœåŠ¡èµ„æº ===")
        
        try:
            if self.query_optimizer:
                await self.query_optimizer.stop_optimizer()
                logger.info("âœ… å›¾æŸ¥è¯¢ä¼˜åŒ–å™¨å·²åœæ­¢å¹¶æ¸…ç†")
            
            if self.db_manager:
                await self.db_manager.disconnect()
                logger.info("âœ… æ•°æ®åº“ç®¡ç†å™¨å·²å…³é—­")
            
            if neo4j_driver._driver:
                await neo4j_driver.close()
                logger.info("âœ… Neo4jè¿æ¥å·²å…³é—­")
            
            logger.info("âœ… æ‰€æœ‰æœåŠ¡èµ„æºæ¸…ç†å®Œæˆ")
            
        except Exception as e:
            logger.error(f"âŒ èµ„æºæ¸…ç†å¤±è´¥: {e}")
    
    async def run_integration_test(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´çš„é›†æˆæµ‹è¯•æµç¨‹"""
        logger.info("\n" + "="*60)
        logger.info("å¼€å§‹æ‰§è¡Œ1.3.2åˆ°1.4.1é›†æˆæµç¨‹æµ‹è¯•")
        logger.info("="*60)
        
        try:
            # åˆå§‹åŒ–æœåŠ¡
            await self.setup_services()
            
            # æ­¥éª¤1: å›¾æŸ¥è¯¢åˆ†æ
            graph_analysis_result = await self.analyze_database_content()
            
            # æ­¥éª¤2: OpenAIæ™ºèƒ½åˆ†æ
            openai_analysis_result = await self.analyze_with_openai(graph_analysis_result)
            
            # æ­¥éª¤3: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = await self.generate_comprehensive_report(
                graph_analysis_result, 
                openai_analysis_result
            )
            
            # è¾“å‡ºæœ€ç»ˆç»“æœ
            logger.info("\n" + "="*60)
            logger.info("é›†æˆæµ‹è¯•å®Œæˆ - æœ€ç»ˆç»“æœ")
            logger.info("="*60)
            
            logger.info("\nğŸ“Š æ‰§è¡Œæ‘˜è¦:")
            exec_summary = comprehensive_report['executive_summary']
            logger.info(f"  â€¢ åˆ†æäº‹ä»¶æ€»æ•°: {exec_summary['total_events_analyzed']}")
            logger.info(f"  â€¢ é«˜ä¼˜å…ˆçº§äº‹ä»¶: {exec_summary['high_priority_events']}")
            logger.info(f"  â€¢ äº‹ä»¶å…³è”æ•°: {exec_summary['event_correlations']}")
            logger.info(f"  â€¢ æ”»å‡»è·¯å¾„æ•°: {exec_summary['attack_paths_detected']}")
            logger.info(f"  â€¢ æ•´ä½“é£é™©è¯„åˆ†: {exec_summary['overall_risk_score']:.1f}/10")
            logger.info(f"  â€¢ å¨èƒç­‰çº§: {exec_summary['threat_level']}")
            
            logger.info("\nğŸ” AIåˆ†æç»“æœ:")
            ai_summary = comprehensive_report['ai_analysis_summary']
            if ai_summary['security_analysis']:
                logger.info(f"  â€¢ å®‰å…¨åˆ†æ: {ai_summary['security_analysis']['summary'][:100]}...")
            if ai_summary['threat_assessment']:
                logger.info(f"  â€¢ å¨èƒè¯„ä¼°: {ai_summary['threat_assessment']['summary'][:100]}...")
            if ai_summary['remediation_advice']:
                logger.info(f"  â€¢ ä¿®å¤å»ºè®®: {ai_summary['remediation_advice']['summary'][:100]}...")
            
            logger.info("\nâš¡ æ€§èƒ½æŒ‡æ ‡:")
            tech_details = comprehensive_report['technical_details']
            logger.info(f"  â€¢ å›¾æŸ¥è¯¢æ‰§è¡Œæ•°: {tech_details['graph_queries_executed']}")
            logger.info(f"  â€¢ AIåˆ†ææ‰§è¡Œæ•°: {tech_details['ai_analyses_performed']}")
            logger.info(f"  â€¢ æ€»å¤„ç†æ—¶é—´: {tech_details['total_processing_time']:.2f}ç§’")
            
            logger.info("\nâœ… é›†æˆæµ‹è¯•æˆåŠŸå®Œæˆ!")
            
            return {
                'status': 'success',
                'test_results': self.test_results,
                'comprehensive_report': comprehensive_report,
                'execution_time': tech_details['total_processing_time']
            }
            
        except Exception as e:
            logger.error(f"\nâŒ é›†æˆæµ‹è¯•å¤±è´¥: {e}")
            return {
                'status': 'failed',
                'error': str(e),
                'partial_results': self.test_results
            }
        
        finally:
            # æ¸…ç†èµ„æº
            await self.cleanup_services()


# æµ‹è¯•å‡½æ•°
@pytest.mark.asyncio
async def test_integration_flow():
    """æµ‹è¯•1.3.2åˆ°1.4.1çš„å®Œæ•´é›†æˆæµç¨‹"""
    test_runner = TestIntegrationFlow()
    result = await test_runner.run_integration_test()
    
    # éªŒè¯æµ‹è¯•ç»“æœ
    assert result['status'] == 'success', f"é›†æˆæµ‹è¯•å¤±è´¥: {result.get('error')}"
    assert 'comprehensive_report' in result
    assert 'test_results' in result
    
    # éªŒè¯å…³é”®ç»„ä»¶
    assert 'graph_analysis' in result['test_results']
    assert 'openai_analysis' in result['test_results']
    assert 'comprehensive_report' in result['test_results']
    
    print("\nğŸ‰ é›†æˆæµç¨‹æµ‹è¯•é€šè¿‡!")
    return result


# ç‹¬ç«‹è¿è¡Œè„šæœ¬
if __name__ == "__main__":
    async def main():
        """ä¸»å‡½æ•°"""
        test_runner = TestIntegrationFlow()
        result = await test_runner.run_integration_test()
        
        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
        import json
        with open('/tmp/integration_test_result.json', 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"\nğŸ“„ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: /tmp/integration_test_result.json")
        return result
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(main())