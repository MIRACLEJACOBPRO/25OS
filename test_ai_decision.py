#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
NeuronOS AIå†³ç­–ä»£ç†æµ‹è¯•è„šæœ¬

è¯¥è„šæœ¬ç”¨äºæµ‹è¯•AIå†³ç­–ä»£ç†çš„å„ä¸ªç»„ä»¶å’Œé›†æˆåŠŸèƒ½ã€‚
æµ‹è¯•å†…å®¹åŒ…æ‹¬ï¼š
1. OpenAIåˆ†ææœåŠ¡é›†æˆ
2. AIå†³ç­–ä»£ç†å†³ç­–é€»è¾‘
3. å‘½ä»¤æ‰§è¡Œå™¨åŠŸèƒ½
4. æ•ˆæœéªŒè¯å™¨åŠŸèƒ½
5. å®Œæ•´å·¥ä½œæµé›†æˆæµ‹è¯•

ä½œè€…: NeuronOS Team
åˆ›å»ºæ—¶é—´: 2024-12-19
"""

import asyncio
import json
import logging
import sys
import os
from datetime import datetime
from typing import Dict, List, Any

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/xzj/01_Project/B_25OS')
sys.path.append('/home/xzj/01_Project/B_25OS/src')
sys.path.append('/home/xzj/01_Project/B_25OS/src/backend')

# å¯¼å…¥æµ‹è¯•æ¨¡å—
try:
    from src.backend.services.ai_decision_integration import AIDecisionIntegration
    from src.backend.services.ai_decision_agent import ExecutionMode
    from src.backend.services.openai_service import AnalysisRequest, AnalysisType
except ImportError as e:
    print(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿é¡¹ç›®è·¯å¾„æ­£ç¡®ä¸”æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
    # å°è¯•ç›´æ¥å¯¼å…¥
    try:
        import services.ai_decision_integration as ai_integration
        import services.ai_decision_agent as ai_agent
        import services.openai_service as openai_svc
        AIDecisionIntegration = ai_integration.AIDecisionIntegration
        ExecutionMode = ai_agent.ExecutionMode
        AnalysisRequest = openai_svc.AnalysisRequest
        AnalysisType = openai_svc.AnalysisType
        print("ä½¿ç”¨å¤‡ç”¨å¯¼å…¥æ–¹å¼æˆåŠŸ")
    except ImportError as e2:
        print(f"å¤‡ç”¨å¯¼å…¥ä¹Ÿå¤±è´¥: {e2}")
        sys.exit(1)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class AIDecisionTester:
    """AIå†³ç­–ä»£ç†æµ‹è¯•å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–æµ‹è¯•å™¨"""
        self.config_path = "/home/xzj/01_Project/B_25OS/config/ai_decision_config.yaml"
        self.integration = None
        self.test_results = []
    
    async def setup(self):
        """è®¾ç½®æµ‹è¯•ç¯å¢ƒ"""
        try:
            logger.info("åˆå§‹åŒ–AIå†³ç­–ä»£ç†é›†æˆæœåŠ¡...")
            self.integration = AIDecisionIntegration(self.config_path)
            logger.info("æµ‹è¯•ç¯å¢ƒè®¾ç½®å®Œæˆ")
            return True
        except Exception as e:
            logger.error(f"è®¾ç½®æµ‹è¯•ç¯å¢ƒå¤±è´¥: {e}")
            return False
    
    def create_test_security_events(self) -> List[Dict[str, Any]]:
        """åˆ›å»ºæµ‹è¯•ç”¨çš„å®‰å…¨äº‹ä»¶"""
        return [
            {
                'event_type': 'suspicious_process',
                'process_name': 'malware.exe',
                'pid': 1234,
                'severity': 'high',
                'timestamp': datetime.now().isoformat(),
                'source': 'endpoint_detection',
                'description': 'æ£€æµ‹åˆ°å¯ç–‘è¿›ç¨‹æ‰§è¡Œ',
                'metadata': {
                    'file_path': '/tmp/malware.exe',
                    'parent_process': 'explorer.exe',
                    'command_line': 'malware.exe --stealth',
                    'network_connections': ['192.168.1.100:4444']
                }
            },
            {
                'event_type': 'network_anomaly',
                'severity': 'medium',
                'timestamp': datetime.now().isoformat(),
                'source': 'network_monitor',
                'description': 'æ£€æµ‹åˆ°å¼‚å¸¸ç½‘ç»œæµé‡',
                'metadata': {
                    'source_ip': '10.0.0.50',
                    'destination_ip': '192.168.1.100',
                    'port': 4444,
                    'protocol': 'TCP',
                    'bytes_transferred': 1048576
                }
            },
            {
                'event_type': 'file_modification',
                'severity': 'low',
                'timestamp': datetime.now().isoformat(),
                'source': 'file_monitor',
                'description': 'ç³»ç»Ÿæ–‡ä»¶è¢«ä¿®æ”¹',
                'metadata': {
                    'file_path': '/etc/hosts',
                    'modification_type': 'content_change',
                    'user': 'root',
                    'process': 'vim'
                }
            }
        ]
    
    async def test_openai_integration(self) -> bool:
        """æµ‹è¯•OpenAIé›†æˆ"""
        logger.info("å¼€å§‹æµ‹è¯•OpenAIé›†æˆ...")
        
        try:
            # åˆ›å»ºæµ‹è¯•äº‹ä»¶
            test_events = self.create_test_security_events()[:1]  # åªç”¨ç¬¬ä¸€ä¸ªäº‹ä»¶
            
            # æµ‹è¯•OpenAIæœåŠ¡
            openai_service = self.integration.openai_service
            
            # åˆ›å»ºåˆ†æè¯·æ±‚
            analysis_request = AnalysisRequest(
                events=test_events,
                analysis_type=AnalysisType.SECURITY_ANALYSIS,
                priority="high",
                context={"test_mode": True}
            )
            
            # æ‰§è¡Œåˆ†æ
            analysis_result = await openai_service.analyze_security_events(analysis_request)
            
            # éªŒè¯ç»“æœ
            if analysis_result and hasattr(analysis_result, 'risk_score'):
                logger.info(f"OpenAIåˆ†ææˆåŠŸï¼Œé£é™©è¯„åˆ†: {analysis_result.risk_score}")
                self.test_results.append({
                    'test': 'openai_integration',
                    'status': 'passed',
                    'details': f'é£é™©è¯„åˆ†: {analysis_result.risk_score}'
                })
                return True
            else:
                logger.error("OpenAIåˆ†æç»“æœæ— æ•ˆ")
                self.test_results.append({
                    'test': 'openai_integration',
                    'status': 'failed',
                    'details': 'åˆ†æç»“æœæ— æ•ˆ'
                })
                return False
                
        except Exception as e:
            logger.error(f"OpenAIé›†æˆæµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                'test': 'openai_integration',
                'status': 'failed',
                'details': str(e)
            })
            return False
    
    async def test_decision_agent(self) -> bool:
        """æµ‹è¯•å†³ç­–ä»£ç†"""
        logger.info("å¼€å§‹æµ‹è¯•å†³ç­–ä»£ç†...")
        
        try:
            # åˆ›å»ºæ¨¡æ‹Ÿåˆ†æç»“æœ
            from src.backend.services.openai_service import AnalysisResponse
            
            from src.backend.services.openai_service import AnalysisType, Priority
            
            mock_analysis = AnalysisResponse(
                request_id="test_request_001",
                analysis_type=AnalysisType.SECURITY_ANALYSIS,
                summary="æ£€æµ‹åˆ°é«˜é£é™©æ¶æ„è¿›ç¨‹",
                detailed_analysis="è¿›ç¨‹malware.exeè¡¨ç°å‡ºå…¸å‹çš„æ¶æ„è½¯ä»¶è¡Œä¸º",
                recommendations=["ç«‹å³ç»ˆæ­¢è¿›ç¨‹", "éš”ç¦»å—å½±å“ç³»ç»Ÿ", "æ‰«æç›¸å…³æ–‡ä»¶"],
                risk_score=85,
                confidence=0.9,
                priority=Priority.HIGH,
                affected_systems=["endpoint-001"],
                attack_vectors=["process_injection", "network_communication"],
                mitigation_steps=["kill_process", "block_ip", "scan_file"],
                timestamp=datetime.now(),
                processing_time=1.5,
                token_usage={'prompt_tokens': 100, 'completion_tokens': 50, 'total_tokens': 150}
            )
            
            # åˆ›å»ºå†³ç­–ä¸Šä¸‹æ–‡
            from src.backend.services.ai_decision_agent import DecisionContext
            
            decision_context = DecisionContext(
                analysis_response=mock_analysis,
                system_state={"test_mode": True},
                execution_mode=ExecutionMode.DRY_RUN,
                user_id="test_user",
                session_id="test_session"
            )
            
            # æ‰§è¡Œå†³ç­–
            decision_agent = self.integration.decision_agent
            decision_result = await decision_agent.make_decision(decision_context)
            
            # éªŒè¯ç»“æœ
            if decision_result and decision_result.execution_plan and hasattr(decision_result.execution_plan, 'decision_type'):
                logger.info(f"å†³ç­–ä»£ç†æˆåŠŸï¼Œå†³ç­–ç±»å‹: {decision_result.execution_plan.decision_type.value}")
                self.test_results.append({
                    'test': 'decision_agent',
                    'status': 'passed',
                    'details': f'å†³ç­–ç±»å‹: {decision_result.execution_plan.decision_type.value}'
                })
                return True
            elif decision_result:
                logger.info(f"å†³ç­–ä»£ç†æˆåŠŸï¼Œä½†æ— æ‰§è¡Œè®¡åˆ’")
                self.test_results.append({
                    'test': 'decision_agent',
                    'status': 'passed',
                    'details': 'å†³ç­–å®Œæˆä½†æ— æ‰§è¡Œè®¡åˆ’'
                })
                return True
            else:
                logger.error("å†³ç­–ä»£ç†ç»“æœæ— æ•ˆ")
                self.test_results.append({
                    'test': 'decision_agent',
                    'status': 'failed',
                    'details': 'å†³ç­–ç»“æœæ— æ•ˆ'
                })
                return False
                
        except Exception as e:
            logger.error(f"å†³ç­–ä»£ç†æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                'test': 'decision_agent',
                'status': 'failed',
                'details': str(e)
            })
            return False
    
    async def test_command_executor(self) -> bool:
        """æµ‹è¯•å‘½ä»¤æ‰§è¡Œå™¨"""
        logger.info("å¼€å§‹æµ‹è¯•å‘½ä»¤æ‰§è¡Œå™¨...")
        
        try:
            # åˆ›å»ºæµ‹è¯•æ‰§è¡Œè®¡åˆ’
            from src.backend.services.ai_decision_agent import ExecutionPlan, DecisionType, RiskLevel
            from src.backend.services.command_executor import CommandType
            
            test_plan = ExecutionPlan(
                plan_id="test_plan",
                decision_type=DecisionType.IMMEDIATE_ACTION,
                risk_level=RiskLevel.LOW,
                commands=[
                    {
                        "action": "check_system_status",
                        "parameters": {"target": "localhost"},
                        "description": "æ£€æŸ¥ç³»ç»ŸçŠ¶æ€"
                    }
                ],
                execution_order=[0],
                estimated_duration=10.0,
                rollback_plan=[],
                approval_required=False,
                created_at=datetime.now(),
                created_by="test_user"
            )
            
            # æ‰§è¡Œå‘½ä»¤ï¼ˆå¹²è¿è¡Œæ¨¡å¼ï¼‰
            decision_agent = self.integration.decision_agent
            execution_results = await decision_agent.execute_plan(test_plan)
            
            # éªŒè¯ç»“æœ
            if execution_results and len(execution_results) > 0:
                logger.info(f"å‘½ä»¤æ‰§è¡Œå™¨æˆåŠŸï¼Œæ‰§è¡Œç»“æœæ•°é‡: {len(execution_results)}")
                self.test_results.append({
                    'test': 'command_executor',
                    'status': 'passed',
                    'details': f'æ‰§è¡Œç»“æœæ•°é‡: {len(execution_results)}'
                })
                return True
            else:
                logger.error("å‘½ä»¤æ‰§è¡Œå™¨ç»“æœæ— æ•ˆ")
                self.test_results.append({
                    'test': 'command_executor',
                    'status': 'failed',
                    'details': 'æ‰§è¡Œç»“æœæ— æ•ˆ'
                })
                return False
                
        except Exception as e:
            logger.error(f"å‘½ä»¤æ‰§è¡Œå™¨æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                'test': 'command_executor',
                'status': 'failed',
                'details': str(e)
            })
            return False
    
    async def test_effect_validator(self) -> bool:
        """æµ‹è¯•æ•ˆæœéªŒè¯å™¨"""
        logger.info("å¼€å§‹æµ‹è¯•æ•ˆæœéªŒè¯å™¨...")
        
        try:
            # åˆ›å»ºæ¨¡æ‹Ÿæ‰§è¡Œç»“æœ
            from src.backend.services.command_executor import ExecutionResult, ExecutionStatus, CommandType
            
            mock_execution = ExecutionResult(
                command_id="test_execution",
                command={
                    "action": "check_system_status",
                    "parameters": {"target": "localhost"}
                },
                status=ExecutionStatus.SUCCESS,
                return_code=0,
                stdout="System status: OK",
                stderr="",
                execution_time=1.5,
                start_time=datetime.now(),
                end_time=datetime.now(),
                error_message=None,
                metadata={"test_mode": True}
            )
            
            # æ‰§è¡Œæ•ˆæœéªŒè¯
            effect_validator = self.integration.effect_validator
            validation_result = await effect_validator.validate_effect(
                command_info={"action": "check_system_status", "parameters": {"target": "localhost"}},
                execution_result=mock_execution
            )
            
            # éªŒè¯ç»“æœ
            if validation_result and hasattr(validation_result, 'is_valid'):
                logger.info(f"æ•ˆæœéªŒè¯å™¨æˆåŠŸï¼ŒéªŒè¯ç»“æœ: {validation_result.is_valid}")
                self.test_results.append({
                    'test': 'effect_validator',
                    'status': 'passed',
                    'details': f'éªŒè¯ç»“æœ: {validation_result.is_valid}'
                })
                return True
            else:
                logger.error("æ•ˆæœéªŒè¯å™¨ç»“æœæ— æ•ˆ")
                self.test_results.append({
                    'test': 'effect_validator',
                    'status': 'failed',
                    'details': 'éªŒè¯ç»“æœæ— æ•ˆ'
                })
                return False
                
        except Exception as e:
            logger.error(f"æ•ˆæœéªŒè¯å™¨æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                'test': 'effect_validator',
                'status': 'failed',
                'details': str(e)
            })
            return False
    
    async def test_full_workflow(self) -> bool:
        """æµ‹è¯•å®Œæ•´å·¥ä½œæµ"""
        logger.info("å¼€å§‹æµ‹è¯•å®Œæ•´å·¥ä½œæµ...")
        
        try:
            # åˆ›å»ºæµ‹è¯•äº‹ä»¶
            test_events = self.create_test_security_events()
            
            # æ‰§è¡Œå®Œæ•´å·¥ä½œæµï¼ˆå¹²è¿è¡Œæ¨¡å¼ï¼‰
            context = await self.integration.process_security_events(
                security_events=test_events,
                user_id="test_user",
                session_id="test_session",
                execution_mode=ExecutionMode.DRY_RUN
            )
            
            # éªŒè¯ç»“æœ
            if context and context.workflow_id:
                logger.info(f"å®Œæ•´å·¥ä½œæµæˆåŠŸï¼Œå·¥ä½œæµID: {context.workflow_id}")
                logger.info(f"æœ€ç»ˆçŠ¶æ€: {context.status.value}")
                
                self.test_results.append({
                    'test': 'full_workflow',
                    'status': 'passed',
                    'details': f'å·¥ä½œæµID: {context.workflow_id}, çŠ¶æ€: {context.status.value}'
                })
                return True
            else:
                logger.error("å®Œæ•´å·¥ä½œæµç»“æœæ— æ•ˆ")
                self.test_results.append({
                    'test': 'full_workflow',
                    'status': 'failed',
                    'details': 'å·¥ä½œæµç»“æœæ— æ•ˆ'
                })
                return False
                
        except Exception as e:
            logger.error(f"å®Œæ•´å·¥ä½œæµæµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                'test': 'full_workflow',
                'status': 'failed',
                'details': str(e)
            })
            return False
    
    async def test_statistics_and_monitoring(self) -> bool:
        """æµ‹è¯•ç»Ÿè®¡å’Œç›‘æ§åŠŸèƒ½"""
        logger.info("å¼€å§‹æµ‹è¯•ç»Ÿè®¡å’Œç›‘æ§åŠŸèƒ½...")
        
        try:
            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.integration.get_statistics()
            
            # è·å–æ´»è·ƒå·¥ä½œæµ
            active_workflows = self.integration.get_active_workflows()
            
            # è·å–å·¥ä½œæµå†å²
            workflow_history = self.integration.get_workflow_history(limit=10)
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            health_status = await self.integration.health_check()
            
            # éªŒè¯ç»“æœ
            if stats and health_status:
                logger.info("ç»Ÿè®¡å’Œç›‘æ§åŠŸèƒ½æ­£å¸¸")
                logger.info(f"ç»Ÿè®¡ä¿¡æ¯: {stats}")
                logger.info(f"å¥åº·çŠ¶æ€: {health_status['status']}")
                
                self.test_results.append({
                    'test': 'statistics_monitoring',
                    'status': 'passed',
                    'details': f'å¥åº·çŠ¶æ€: {health_status["status"]}'
                })
                return True
            else:
                logger.error("ç»Ÿè®¡å’Œç›‘æ§åŠŸèƒ½å¼‚å¸¸")
                self.test_results.append({
                    'test': 'statistics_monitoring',
                    'status': 'failed',
                    'details': 'åŠŸèƒ½å¼‚å¸¸'
                })
                return False
                
        except Exception as e:
            logger.error(f"ç»Ÿè®¡å’Œç›‘æ§åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            self.test_results.append({
                'test': 'statistics_monitoring',
                'status': 'failed',
                'details': str(e)
            })
            return False
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        logger.info("å¼€å§‹è¿è¡ŒAIå†³ç­–ä»£ç†å®Œæ•´æµ‹è¯•å¥—ä»¶...")
        
        # è®¾ç½®æµ‹è¯•ç¯å¢ƒ
        if not await self.setup():
            return {
                'success': False,
                'error': 'æµ‹è¯•ç¯å¢ƒè®¾ç½®å¤±è´¥',
                'results': []
            }
        
        # å®šä¹‰æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            ('OpenAIé›†æˆæµ‹è¯•', self.test_openai_integration),
            ('å†³ç­–ä»£ç†æµ‹è¯•', self.test_decision_agent),
            ('å‘½ä»¤æ‰§è¡Œå™¨æµ‹è¯•', self.test_command_executor),
            ('æ•ˆæœéªŒè¯å™¨æµ‹è¯•', self.test_effect_validator),
            ('å®Œæ•´å·¥ä½œæµæµ‹è¯•', self.test_full_workflow),
            ('ç»Ÿè®¡ç›‘æ§æµ‹è¯•', self.test_statistics_and_monitoring)
        ]
        
        # æ‰§è¡Œæµ‹è¯•
        passed_tests = 0
        total_tests = len(test_cases)
        
        for test_name, test_func in test_cases:
            logger.info(f"\n{'='*50}")
            logger.info(f"æ‰§è¡Œæµ‹è¯•: {test_name}")
            logger.info(f"{'='*50}")
            
            try:
                result = await test_func()
                if result:
                    passed_tests += 1
                    logger.info(f"âœ… {test_name} - é€šè¿‡")
                else:
                    logger.error(f"âŒ {test_name} - å¤±è´¥")
            except Exception as e:
                logger.error(f"âŒ {test_name} - å¼‚å¸¸: {e}")
        
        # ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š
        success_rate = passed_tests / total_tests
        test_summary = {
            'success': success_rate == 1.0,
            'total_tests': total_tests,
            'passed_tests': passed_tests,
            'failed_tests': total_tests - passed_tests,
            'success_rate': success_rate,
            'test_time': datetime.now().isoformat(),
            'results': self.test_results
        }
        
        logger.info(f"\n{'='*60}")
        logger.info("æµ‹è¯•æ€»ç»“")
        logger.info(f"{'='*60}")
        logger.info(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        logger.info(f"é€šè¿‡æµ‹è¯•: {passed_tests}")
        logger.info(f"å¤±è´¥æµ‹è¯•: {total_tests - passed_tests}")
        logger.info(f"æˆåŠŸç‡: {success_rate:.2%}")
        
        if success_rate == 1.0:
            logger.info("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼AIå†³ç­–ä»£ç†åŠŸèƒ½æ­£å¸¸")
        else:
            logger.warning("âš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç›¸å…³ç»„ä»¶")
        
        return test_summary
    
    def save_test_report(self, test_summary: Dict[str, Any], filename: str = None):
        """ä¿å­˜æµ‹è¯•æŠ¥å‘Š"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"/home/xzj/01_Project/B_25OS/ai_decision_test_report_{timestamp}.json"
        
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(test_summary, f, indent=2, ensure_ascii=False)
            logger.info(f"æµ‹è¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {filename}")
        except Exception as e:
            logger.error(f"ä¿å­˜æµ‹è¯•æŠ¥å‘Šå¤±è´¥: {e}")

async def main():
    """ä¸»å‡½æ•°"""
    print("NeuronOS AIå†³ç­–ä»£ç†æµ‹è¯•è„šæœ¬")
    print("=" * 50)
    
    # åˆ›å»ºæµ‹è¯•å™¨
    tester = AIDecisionTester()
    
    try:
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        test_summary = await tester.run_all_tests()
        
        # ä¿å­˜æµ‹è¯•æŠ¥å‘Š
        tester.save_test_report(test_summary)
        
        # è¿”å›ç»“æœ
        if test_summary['success']:
            print("\nğŸ‰ æµ‹è¯•å®Œæˆï¼æ‰€æœ‰åŠŸèƒ½æ­£å¸¸")
            return 0
        else:
            print("\nâš ï¸ æµ‹è¯•å®Œæˆï¼éƒ¨åˆ†åŠŸèƒ½å¼‚å¸¸")
            return 1
            
    except KeyboardInterrupt:
        print("\nç”¨æˆ·ä¸­æ–­æµ‹è¯•")
        return 130
    except Exception as e:
        print(f"\næµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
        return 1

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    exit_code = asyncio.run(main())
    sys.exit(exit_code)